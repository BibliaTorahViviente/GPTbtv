@@ -0,0 +1,35 @@
from collections import namedtuple


ModelInfo = namedtuple("ModelInfo", ["simple_name", "link", "description"]) 


model_info = {

}


def register_model_info(full_names, simple_name, link, description):
    info = ModelInfo(simple_name, link, description)

    for full_name in full_names:
        model_info[full_name] = info


def get_model_info(name):
    return model_info[name]


register_model_info(["gpt-4"], "ChatGPT-4", "https://chat.openai.com/", "ChatGPT-4 by OpenAI")
register_model_info(["gpt-3.5-turbo"], "ChatGPT-3.5", "https://chat.openai.com/", "ChatGPT-3.5 by OpenAI")
register_model_info(["claude-v1"], "Claude", "https://www.anthropic.com/index/introducing-claude", "Claude by Anthropic")
register_model_info(["vicuna-13b"], "Vicuna", "https://lmsys.org/blog/2023-03-30-vicuna/", "a chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS")
register_model_info(["koala-13b"], "Koala", "https://bair.berkeley.edu/blog/2023/04/03/koala", "a dialogue model for academic research by BAIR")
register_model_info(["oasst-pythia-12b"], "OpenAssistant", "https://open-assistant.io", "an Open Assistant for everyone by LAION")
register_model_info(["RWKV-4-Raven-14B"], "RMKV-4-Raven", "https://huggingface.co/BlinkDL/rwkv-4-raven", "an RNN with transformer-level LLM performance")
register_model_info(["alpaca-13b"], "Alpaca", "https://crfm.stanford.edu/2023/03/13/alpaca.html", "a model fine-tuned from LLaMA on instruction-following demonstrations by Stanford")
register_model_info(["chatglm-6b"], "ChatGLM", "https://chatglm.cn/blog", "an open bilingual dialogue language model by Tsinghua University")
register_model_info(["llama-13b"], "LLaMA", "https://arxiv.org/abs/2302.13971", "open and efficient foundation language models by Meta")
register_model_info(["dolly-v2-12b"], "Dolly", "https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm", "an instruction-tuned open large language model by Databricks")
register_model_info(["stablelm-tuned-alpha-7b"], "StableLM", "https://github.com/stability-AI/stableLM", "Stability AI language models")
register_model_info(["fastchat-t5-3b"], "FastChat-T5", "https://huggingface.co/lmsys/fastchat-t5-3b-v1.0", "a chat assistant fine-tuned from FLAN-T5 by LMSYS")
  27 changes: 6 additions & 21 deletions27  
fastchat/serve/gradio_web_server.py
@@ -18,15 +18,17 @@
    SeparatorStyle,
)
from fastchat.constants import LOGDIR

from fastchat.serve.gradio_patch import Chatbot as grChatbot
from fastchat.serve.gradio_css import code_highlight_css
from fastchat.model.model_registry import model_info
from fastchat.utils import (
    build_logger,
    server_error_msg,
    violates_moderation,
    moderation_msg,
    get_window_url_params_js,
)
from fastchat.serve.gradio_patch import Chatbot as grChatbot
from fastchat.serve.gradio_css import code_highlight_css


logger = build_logger("gradio_web_server", "gradio_web_server.log")
@@ -40,23 +42,6 @@
controller_url = None
enable_moderation = False


model_info = {
    "gpt-4": ("ChatGPT-4", "https://chat.openai.com/", "ChatGPT-4 by OpenAI"),
    "gpt-3.5-turbo": ("ChatGPT-3.5", "https://chat.openai.com/", "ChatGPT-3.5 by OpenAI"),
    "claude-v1": ("Claude", "https://www.anthropic.com/index/introducing-claude", "Claude by Anthropic"),
    "vicuna-13b": ("Vicuna", "https://lmsys.org/blog/2023-03-30-vicuna/", "a chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS"),
    "koala-13b": ("Koala", "https://bair.berkeley.edu/blog/2023/04/03/koala", "a dialogue model for academic research by BAIR"),
    "oasst-pythia-12b": ("OpenAssistant", "https://open-assistant.io", "an Open Assistant for everyone by LAION"),
    "RWKV-4-Raven-14B": ("RMKV-4-Raven", "https://huggingface.co/BlinkDL/rwkv-4-raven", "an RNN with transformer-level LLM performance"),
    "alpaca-13b": ("Alpaca", "https://crfm.stanford.edu/2023/03/13/alpaca.html", "a model fine-tuned from LLaMA on instruction-following demonstrations by Stanford"),
    "chatglm-6b": ("ChatGLM", "https://chatglm.cn/blog", "an open bilingual dialogue language model by Tsinghua University"),
    "llama-13b": ("LLaMA", "https://arxiv.org/abs/2302.13971", "open and efficient foundation language models by Meta"),
    "dolly-v2-12b": ("Dolly", "https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm", "an instruction-tuned open large language model by Databricks"),
    "stablelm-tuned-alpha-7b": ("StableLM", "https://github.com/stability-AI/stableLM", "Stability AI language models"),
    "fastchat-t5-3b": ("FastChat-T5", "https://huggingface.co/lmsys/fastchat-t5-3b-v1.0", "a chat assistant fine-tuned from FLAN-T5 by LMSYS"),
}

learn_more_md = """
### License
The service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.
@@ -433,8 +418,8 @@ def build_single_model_ui(models):
            model_description_md += "|"

        if name in model_info:
            name, link, desc = model_info[name]
            model_description_md += f" [{name}]({link}): {desc} |"
            minfo = model_info[name]
            model_description_md += f" [{name}]({minfo.link}): {minfo.description} |"
        else:
            model_description_md += f" |"
        if i % 3 == 2:
  26 changes: 20 additions & 6 deletions26  
fastchat/serve/monitor/basic_stats.py
@@ -1,3 +1,4 @@
import argparse
import code
import datetime
import json
@@ -28,11 +29,19 @@ def get_log_files(max_num_files=None):


def load_log_files(log_files):
    dfs = []
    for filename in tqdm(log_files):
        dfs.append(pd.read_json(filename, lines=True))
    df = pd.concat(dfs).reset_index(drop=True)
    return df
    data = []
    for filename in tqdm(log_files, desc="read files"):
        for l in open(filename):
            row = json.loads(l)

            data.append(dict(
                type=row["type"],
                tstamp=row["tstamp"],
                model=row.get("model", ""),
                models=row.get("models", ["", ""]),
            ))

    return data


def merge_counts(series, on, names):
@@ -50,6 +59,7 @@ def merge_counts(series, on, names):

def report_basic_stats(log_files):
    df_all = load_log_files(log_files)
    df_all = pd.DataFrame(df_all)
    now_t = df_all["tstamp"].max()

    df_1_hour = df_all[df_all["tstamp"] > (now_t - 3600)]
@@ -113,7 +123,11 @@ def report_basic_stats(log_files):


if __name__ == "__main__":
    log_files = get_log_files()
    parser = argparse.ArgumentParser()
    parser.add_argument("--max-num-files", type=int)
    args = parser.parse_args()

    log_files = get_log_files(args.max_num_files)
    basic_stats = report_basic_stats(log_files)

    print(basic_stats["model_hist_md"] + "\n")
  16 changes: 8 additions & 8 deletions16  
fastchat/serve/monitor/clean_battle_data.py
@@ -59,13 +59,11 @@ def remove_html(raw):

def clean_battle_data(log_files):
    data = []
    for filename in tqdm(log_files):
        with open(filename) as f:
            lines = f.readlines()
        for l in lines:
            dp = json.loads(l)
            if dp["type"] in VOTES:
                data.append(dp)
    for filename in tqdm(log_files, desc="read files"):
        for l in open(filename):
            row = json.loads(l)
            if row["type"] in VOTES:
                data.append(row)

    convert_type = {
        "leftvote": "model_a",
@@ -111,6 +109,7 @@ def clean_battle_data(log_files):
            ct_invalid += 1
            continue
        lang_code = detect_lang(state["messages"][state["offset"]][1])
        rounds = (len(state["messages"]) - state["offset"]) // 2

        # Drop conversations if the model names are leaked
        leaked_identity = False
@@ -135,8 +134,9 @@ def clean_battle_data(log_files):
            model_b=models[1],
            win=convert_type[row["type"]],
            anony=anony,
            tstamp=row["tstamp"],
            rounds=rounds,
            language=lang_code,
            tstamp=row["tstamp"],
        ))

        all_models.update(models_hidden)
  103 changes: 55 additions & 48 deletions103  
fastchat/serve/monitor/elo_analysis.py
@@ -1,18 +1,20 @@
import argparse
from collections import defaultdict
import datetime
import json
import math
import pickle
from pytz import timezone

import gdown
import numpy as np
import pandas as pd
import plotly.express as px
from tqdm import tqdm

from fastchat.model.model_registry import get_model_info
from fastchat.serve.monitor.basic_stats import get_log_files
from fastchat.serve.monitor.clean_battle_data import clean_battle_data
from fastchat.serve.gradio_web_server import model_info


pd.options.display.float_format = "{:.2f}".format
@@ -40,55 +42,16 @@ def compute_elo(battles, K=32, SCALE=400, BASE=10, INIT_RATING=1000):
    return dict(rating)


def get_bootstrap_result(battles, func_compute_elo, num_round):
def get_bootstrap_result(battles, func_compute_elo, num_round=1000):
    rows = []
    for i in tqdm(range(num_round), desc="bootstrap"):
        rows.append(func_compute_elo(battles.sample(frac=1.0, replace=True)))
    df = pd.DataFrame(rows)
    return df[df.median().sort_values(ascending=False).index]


def visualize_leaderboard_md(rating):
    models = list(rating.keys())
    models.sort(key=lambda k: -rating[k])

    emoji_dict = {
        1: "ðŸ¥‡",
        2: "ðŸ¥ˆ",
        3: "ðŸ¥‰",
    }

    md = """
# Leaderboard
[[Blog](https://lmsys.org/blog/2023-05-03-arena/)] [[GitHub]](https://github.com/lm-sys/FastChat) [[Twitter]](https://twitter.com/lmsysorg) [[Discord]](https://discord.gg/h6kCZb72G7)
We use the Elo rating system to calculate the relative performance of the models. You can view the voting data, basic analyses, and calculation procedure in this [notebook](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing). The current leaderboard is based on the data we collected before May 1, 2023. We will periodically release new leaderboards.\n
"""
    md += "| Rank | Model | Elo Rating | Description |\n"
    md += "| --- | --- | --- | --- |\n"
    for i, model in enumerate(models):
        rank = i + 1
        _, link, desc = model_info[model]
        emoji = emoji_dict.get(rank, "")
        md += f"| {rank} | {emoji} [{model}]({link}) | {rating[model]:.0f} | {desc} |\n"

    return md

def visualize_bootstrap_elo_rating(battles, num_round=1000):
    df = get_bootstrap_result(battles, compute_elo, num_round)

    bars = pd.DataFrame(dict(
        lower = df.quantile(.025),
        rating = df.quantile(.5),
        upper = df.quantile(.975))).reset_index(names="model").sort_values("rating", ascending=False)
    bars['error_y'] = bars['upper'] - bars["rating"]
    bars['error_y_minus'] = bars['rating'] - bars["lower"]
    bars['rating_rounded'] = np.round(bars['rating'], 2)
    fig = px.scatter(bars, x="model", y="rating", error_y="error_y", 
                     error_y_minus="error_y_minus", text="rating_rounded",
                     width=450)
    fig.update_layout(xaxis_title="Model", yaxis_title="Rating")
    return fig
def get_elo_from_bootstrap(bootstrap_df):
    return dict(bootstrap_df.quantile(.5))


def compute_pairwise_win_fraction(battles, model_order):
@@ -120,7 +83,44 @@ def compute_pairwise_win_fraction(battles, model_order):
    # Arrange ordering according to proprition of wins
    row_beats_col = row_beats_col_freq.loc[model_order, model_order]
    return row_beats_col



def visualize_leaderboard_table(rating):
    models = list(rating.keys())
    models.sort(key=lambda k: -rating[k])

    emoji_dict = {
        1: "ðŸ¥‡",
        2: "ðŸ¥ˆ",
        3: "ðŸ¥‰",
    }

    md = ""
    md += "| Rank | Model | Elo Rating | Description |\n"
    md += "| --- | --- | --- | --- |\n"
    for i, model in enumerate(models):
        rank = i + 1
        minfo = get_model_info(model)
        emoji = emoji_dict.get(rank, "")
        md += f"| {rank} | {emoji} [{model}]({minfo.link}) | {rating[model]:.0f} | {minfo.description} |\n"

    return md


def visualize_bootstrap_elo_rating(df):
    bars = pd.DataFrame(dict(
        lower = df.quantile(.025),
        rating = df.quantile(.5),
        upper = df.quantile(.975))).reset_index(names="model").sort_values("rating", ascending=False)
    bars['error_y'] = bars['upper'] - bars["rating"]
    bars['error_y_minus'] = bars['rating'] - bars["lower"]
    bars['rating_rounded'] = np.round(bars['rating'], 2)
    fig = px.scatter(bars, x="model", y="rating", error_y="error_y", 
                     error_y_minus="error_y_minus", text="rating_rounded",
                     width=450)
    fig.update_layout(xaxis_title="Model", yaxis_title="Rating")
    return fig


def visualize_pairwise_win_fraction(battles, model_order):
    row_beats_col = compute_pairwise_win_fraction(battles, model_order)
@@ -167,25 +167,31 @@ def report_elo_analysis_results(battles_json):
    battles = battles[battles["anony"]].reset_index(drop=True)
    battles_no_ties = battles[~battles["win"].str.contains("tie")]

    elo_rating = compute_elo(battles)
    bootstrap_df = get_bootstrap_result(battles, compute_elo)
    elo_rating = get_elo_from_bootstrap(bootstrap_df)
    elo_rating = {k: int(v) for k, v in elo_rating.items()}

    model_order = list(elo_rating.keys())
    model_order.sort(key=lambda k: -elo_rating[k])

    leaderboard_md = visualize_leaderboard_md(elo_rating)
    leaderboard_table = visualize_leaderboard_table(elo_rating)
    win_fraction_heatmap = visualize_pairwise_win_fraction(battles_no_ties, model_order)
    battle_count_heatmap = visualize_battle_count(battles_no_ties, model_order)
    average_win_rate_bar = visualize_average_win_rate(battles_no_ties)
    bootstrap_elo_rating = visualize_bootstrap_elo_rating(battles)
    bootstrap_elo_rating = visualize_bootstrap_elo_rating(bootstrap_df)

    last_update_tstamp = battles["tstamp"].max()
    last_update_datetime = datetime.datetime.fromtimestamp(last_update_tstamp,
        tz=timezone('US/Pacific')).strftime("%Y-%m-%d %H:%M:%S %Z")

    return {
        "elo_rating": elo_rating,
        "leaderboard_md": leaderboard_md,
        "leaderboard_table": leaderboard_table,
        "win_fraction_heatmap": win_fraction_heatmap,
        "battle_count_heatmap": battle_count_heatmap,
        "average_win_rate_bar": average_win_rate_bar,
        "bootstrap_elo_rating": bootstrap_elo_rating,
        "last_update_datetime": f"{last_update_datetime}",
    }


@@ -213,6 +219,7 @@ def pretty_print_elo_rating(rating):
    results = report_elo_analysis_results(battles)

    pretty_print_elo_rating(results["elo_rating"])
    print(f"last update : {results['last_update_datetime']}")

    with open("elo_results.pkl", "wb") as fout:
        pickle.dump(results, fout)
  14 changes: 12 additions & 2 deletions14  
fastchat/serve/monitor/monitor.py
@@ -1,3 +1,6 @@
# sudo apt install pkg-config libicu-dev
# pip install pytz gradio gdown plotly polyglot pyicu pycld2 tabulate

import argparse
import datetime
import pickle
@@ -34,7 +37,14 @@ def update_elo_components(max_num_files, elo_results_file):
    if elo_results_file is None:
        battles = clean_battle_data(log_files)
        elo_results = report_elo_analysis_results(battles)
        leader_component_values[0] = elo_results["leaderboard_md"]
        leaderboard_md = f"""
# Leaderboard
[[Blog](https://lmsys.org/blog/2023-05-03-arena/)] [[GitHub]](https://github.com/lm-sys/FastChat) [[Twitter]](https://twitter.com/lmsysorg) [[Discord]](https://discord.gg/h6kCZb72G7)
We use the Elo rating system to calculate the relative performance of the models. You can view the voting data, basic analyses, and calculation procedure in this [notebook](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing).
Last update: {elo_results["last_update_datetime"]}
"""
        leader_component_values[0] = leaderboard_md + elo_results["leaderboard_table"]
        leader_component_values[1] = elo_results["win_fraction_heatmap"]
        leader_component_values[2] = elo_results["battle_count_heatmap"]
        leader_component_values[3] = elo_results["average_win_rate_bar"]
@@ -140,7 +150,7 @@ def build_demo(elo_results_file):
    parser.add_argument("--port", type=int)
    parser.add_argument("--share", action="store_true")
    parser.add_argument("--concurrency-count", type=int, default=10)
    parser.add_argument("--update-interval", type=int, default=600)
    parser.add_argument("--update-interval", type=int, default=300)
    parser.add_argument("--max-num-files", type=int)
    parser.add_argument("--elo-results-file", type=str)
    args = parser.parse_args()
  8 changes: 4 additions & 4 deletions8  
pyproject.toml
@@ -13,10 +13,10 @@ classifiers = [
    "License :: OSI Approved :: Apache Software License",
]
dependencies = [
    "accelerate", "fastapi", "gradio==3.23", "markdown2[all]", "numpy",
    "prompt_toolkit>=3.0.0", "requests", "rich>=10.0.0", "sentencepiece",
    "shortuuid", "transformers>=4.28.0,<4.29.0", "tokenizers>=0.12.1", "torch",
    "uvicorn", "wandb", "httpx", "shortuuid", "pydantic", "nh3",
    "accelerate", "fastapi", "gradio==3.23", "httpx", "markdown2[all]", "nh3", "numpy",
    "prompt_toolkit>=3.0.0", "pydantic", "requests", "rich>=10.0.0", "sentencepiece",
    "shortuuid", "shortuuid", "tokenizers>=0.12.1", "torch",
    "transformers>=4.28.0,<4.29.0", "uvicorn", "wandb"
]

[project.optional-dependencies] V 
