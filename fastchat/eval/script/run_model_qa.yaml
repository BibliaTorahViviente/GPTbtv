resources:
  accelerators: A100:4
  cloud: gcp

num_nodes: 1

workdir: .

setup: |
  conda activate chatbot
  if [ $? -eq 0 ]; then
    echo 'conda env exists'
  else
    # Setup the environment
    conda create -n chatbot python=3.10 -y
  fi
  conda activate chatbot

  pip3 install -e .

  # Install pytorch
  pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116

  # Install huggingface with the LLaMA commit
  pip install git+https://github.com/huggingface/transformers

  cd fastchat/eval
  pip install -r requirements.txt

  MODEL_NAME=vicuna-13b-20230322-new-hp-fp16
  MODEL_PATH=~/${MODEL_NAME}

  if [ ! -f "$MODEL_PATH/ready" ]; then
    echo "export MODEL_PATH=${MODEL_PATH}" >> ~/.bashrc
    echo "export MODEL_NAME=${MODEL_NAME}" >> ~/.bashrc
    mkdir -p $MODEL_PATH
    gsutil -m cp gs://model-weights/${MODEL_NAME}/* $MODEL_PATH
    touch $MODEL_PATH/ready
    echo "model downloaded"
  fi

run: |
  python -m fastchat.eval.get_model_answer --model_path $MODEL_PATH \
    --model-id $MODEL_NAME \
    --question fastchat/eval/table/question.jsonl \
    --answer answer.jsonl \
    --num-gpus $SKYPILOT_NUM_GPUS_PER_NODE
